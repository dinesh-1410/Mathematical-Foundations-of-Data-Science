# -*- coding: utf-8 -*-
"""BE21B032_CH5019_PROJECT_Q1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1--bBHOnTNYu39Q3wrxRUTj9cVZEIVYNl

# **IMPORTING NECESSARY LIBRARIES**
"""

import numpy as np
import pandas as pd

# Finxing a random seed value for reproducibility
np.random.seed(5)

"""# **LINEAR REGRESSION MODEL**

In Linear Regression Model, We can write the relationship between independent and identically distributed (i.i.d) Response Varaibles (Y) and Covariate Varaibles (X) as:

$$ y_i = x_i^T \beta + \epsilon_i $$

where, $\beta_i,$ i = 1,...,p are the unknown parameters and $\epsilon_i$ is error of the equation & are i.i.d and independent of $x_i$ with $ E(\epsilon_i|x_i) = 0 $.

 The most commonly used estimate for $ \beta $ is the ordinary least-square (OLS) estimate that minimizes the sum of squared residuals

$$ OLS:  \hat{\beta} = \underset{\beta}{\mathrm{argmin}} \, \sum_{i=1}^{n} (y_i - x_i^T \beta)^2 $$

Least Median of Squares:
 The LMS estimates are found by minimizing the median of the squared residuals

$$LMS : \hat{\beta} = \underset{\beta}{\mathrm{argmin}} \, \mathrm{Med} (y_i - x_i^T \beta)^2$$

 Least Trimmed Squares:
 The LTS estimate is defined as
$$ LTS : \hat{\beta} = \underset{\beta}{\mathrm{argmin}} \, \sum_{i=1}^{q} r(i)(\beta)^2 $$

where $ r(1)(\beta)^2 \leq \ldots \leq r(q)(\beta)^2 $ are ordered squared residuals, $q = (\frac{n}{2} + 1)$ and $n$ is the number of samples

""" # Defining a method to compute the errors (costs) of the model parameters (beta) for each method of estimation
    def compute_cost(self, X, y, beta):
        # Number of Samples
        n = len(y)
        # Predictions of the Linear Regression Model given by the below dot product between Covariate Matrix and Parameters Matrix
        preds = X.dot(beta)
        # The difference between the actual output and the model’s predictions (Residuals)
        resids = (y - preds)**2
        # Defining Objective Cost (Loss) Functions for each Method of Estimation
        # For Ordinary Least Squares (OLS) Method of Estimation
        if self.method == 'OLS':
            cost = np.sum(resids)
        # For Least Median Squares (LMS) Method of Estimation
        elif self.method == 'LMS':
            cost = np.median(resids)
        # For Least Trimmed Squares (LTS) Method of Estimation
        elif self.method == 'LTS':
            resids_sorted = np.sort(resids, axis=0)
            q = int((n / 2) + 1)
            cost = np.sum(resids_sorted[:q])
        return cost
"""

# Defining a Class for the Linear Regression Model
class LinearRegression:
    def __init__(self, method, l_r, iter):
        # Initializing the hyper-parameters of the Linear Regression Model like method of estimation, Learning_rate, Number of Iterations
        self.method = method
        self.l_r = l_r
        self.iter = iter
        # Initializing the parameters of the Linear Regression Model like beta, cost
        self.beta = None
        self.beta_hist = None

    # Defining a method to compute the gradients of the Cost (Loss) Function with respect to model parameters (beta) for each method of estimation
    def compute_gradient(self, X, Y, beta):
        # Number of Samples
        n,p = X.shape
        # Gradient of the Residuals term
        resid = (Y - X.dot(beta))
        # Defining Gradients of the Cost (Loss) Function with respect to model parameters (beta) for each Method of Estimation
        # For Ordinary Least Squares (OLS) Method of Estimation
        if self.method == 'OLS':
            grads = (-2) * (X.T).dot(resid)
            grads = np.clip(grads, -1e6, 1e6)
        # For Least Median Squares (LMS) Method of Estimation

        elif self.method == 'LMS':
            sqr_res = resid**2
            sqr_res_sorted = np.sort(sqr_res)

            # Calculate median
            n = len(sqr_res_sorted)
            if n % 2 == 0:
                med = (sqr_res_sorted[n//2] + sqr_res_sorted[n//2 - 1]) / 2
            else:
                med = sqr_res_sorted[n//2]
            # Find the position of the element closest to the median
            pos = np.abs(sqr_res - med).argmin() % X.shape[1]
            grads = (-2) * (X.T[:,pos]) * (Y[pos,:] - np.matmul((X[pos,:]),(beta)))
            grads = grads.reshape(-1,1)
            grads = np.clip(grads, -1e6, 1e6)

        # For Least Trimmed Squares (LTS) Method of Estimation
        elif self.method == 'LTS':
            sqr_res = (resid)**2
            sqr_res_sorted = np.sort(sqr_res, axis=0)
            q = int((n / 2) + 1)
            # Creating a mask for residuals that are not in the smallest half
            mask = sqr_res > sqr_res_sorted[q]
            # Treating these residuals as zero when calculating the gradient
            masked_resid = np.where(mask, 0, resid)
            grads = (-2) * (X.T).dot(masked_resid)
            grads = np.clip(grads, -1e6, 1e6)
        return grads


    # Defining a method to perform the gradientd descent algorithm of the Cost (Loss) Function with respect to model parameters (beta) for each method of estimation
    def gradient_descent(self, X, Y, beta):
        # Creating Arrays to store the beta values at each iteration
        self.beta_hist = np.zeros((self.iter, len(beta)))
        # Creating a for loop to loop over the number of iterations
        for i in range(self.iter):
            # Calculate the gradient of the Cost (Loss) function with respect to beta
            grads = self.compute_gradient(X, Y, beta)
            # Updating the value of the estimated parameter beta by subtracting the product of the learning rate and the gradients from the current value of beta.
            beta = beta - (self.l_r*grads)
            # storing the current values of beta in the above defined arrays
            self.beta_hist[i,:] = beta.T
        return beta

    # Defining a method to estimate the unknown parameters (beta) of the model (beta) for each method of estimation
    def fit(self, X, Y):
        n, p = X.shape
        # Initializing the parameter vector beta with random values & with size of the number of features in X
        self.beta = np.random.randn(p,1)
        # Optimizing the parameters beta by using the gradientd descent algorithm
        self.beta = self.gradient_descent(X, Y, self.beta)

"""# **TESTING ON SYNTHETIC DATA**

Considering the two covariate variables, $X_i$, where $i = 1,2$ following a standard normal distribution $N(0,1)$, and noise $ϵ$ from a normal distribution $N(0,1)$.
By Using OLS, LMS, and LTS methods to estimate the model's parameters for $N = 20$ and $N = 100$ observations with $R = 200$ realizations.
ALso, By Comparing the estimates using given metrics and present the results in tables for $N = 20$ and $N = 100$ separately.

Metrics for Comparison:

$${Mean Square Error (MSE)}:MSE_i = bias(\hat{\beta}_i)^2 + var(\hat{\beta}_i)$$

$${Robust Bias (RB)}: RB_i = median(\hat{\beta}_i) - \beta_i$$

$$ {Median Absolute Deviation (MAD)}: MAD_i = median(|\hat{\beta}_i -\beta_i|)$$

where $\hat{\beta}_i$ and $\beta_i$ are the parameter estimate and true parameter respectively, and $i = 0,1,...,p$ are the number of regressors.
"""

# Defining a function to generate Synthetic Data of Covariate Matrix and Response Matrix
def generate_data(N, beta_true):
    #  Generating a 2D array pf Covariate Matirx X of size N x len(beta_true) following a standard normal distribution with mean = 0 and standard deviation = 1
    X = np.random.normal(0, 1, (N, len(beta_true)))
    #  Generating a 2D array of noise of size N following a standard normal distribution with mean = 0 and standard deviation = 1
    noise = np.random.normal(0, 1, (N, 1))
    #  Generating the Response (Dependent) variable Matrix Y in a linear regression model
    Y = (X.dot(beta_true)) + noise
    return X, Y

# Defining a function to calculate the 3 Metrics Defined above (MSE, RB, MAD) for all 3 methods of estimation (OLS, LMS, LTS)
def compute_metrics(beta_est, beta_true, R):
    # Calculating the Mean Square Error (MSE) by taking square of the difference between the true parameters from the estimated parameters, and the mean across the rows (axis = 1)
    MSE = np.mean((beta_est - beta_true.flatten())**2, axis=0)
    # Calculating the Robust Bias (RB) by getting the median of the estimated parameters across the rows (axis = 1) and subtracting the true parameters
    RB = np.median(beta_est, axis=0) - beta_true.flatten()
    # Calculating the Mean Absolute Deviation (MAD) by taking absolute value of the difference between the true parameters from the estimated parameters, and by computing median across the rows (axis = 1)
    MAD = np.median(np.abs(beta_est - beta_true.flatten()), axis=0)
    return MSE, RB, MAD

# Defining The Number of Samples
N_values = [20, 100]
# Defining The Number of Realisations
R = 200
# Defining The True (Actual) Values of the Parameters (Beta)
beta_true = np.array([[3], [5]])
# Defining The Lists to store the values of parameters and the metrics for each method of estimation
beta_results = []
metric_results = []

# Defining a For loop to loop over the different numbers of samples
for N in N_values:
    # Defining a Nested For Loop to loop over every 3 methods of estimation ('OLS', 'LMS', 'LTS')
    for method in ['OLS', 'LMS', 'LTS']:
        # intializing beta_estimator matrix for all realisations to shape of number of Realisations x Covariates
        beta_est = np.zeros((R, len(beta_true)))
        # Defining a Nested For Loop to loop over every realisation
        for r in range(R):
            # Generating Synthetic Data for Covariate Matrix (X) and Response Matrix (Y)
            X_syn, Y_syn = generate_data(N, beta_true)
            # Creating an instance of the LinearRegression class with a specified method of estimation
            lr = LinearRegression(method=method,l_r = 0.0005, iter=1000)
            # Fitting the linear regression model to the data X and Y using the specified method of estimation
            lr.fit(X_syn, Y_syn)
            # Storing the estimated parameters from the fitted model for that realisation 'r'
            beta_est[r, :] = lr.beta.ravel()
        # Appending the average of the estimated parameters over all realisations for each method of estimation to the list
        beta_results.append([N, method, np.mean(beta_est[:,0]), np.mean(beta_est[:,1])])
        # Calculating the Mean Square Error (MSE), Robust Bias (RB), and Median Absolute Deviation (MAD) for the estimated parameters
        MSE, RB, MAD = compute_metrics(beta_est, beta_true, R)
        # Appending the Metrics for each method of estimation to the list
        metric_results.append([N, method, MSE[0], MSE[1], RB[0], RB[1], MAD[0], MAD[1]])

# Create DataFrames for beta results and metric results for each method of estimation
beta_cost_df = pd.DataFrame(beta_results, columns=['N', 'Method', 'Beta0', 'Beta1'])
metric_df = pd.DataFrame(metric_results, columns=['N', 'Method', 'MSE Beta0', 'MSE Beta1', 'RB Beta0', 'RB Beta1', 'MAD Beta0', 'MAD Beta1'])

# Print beta table for each method of estimation
print("Beta Table: for each method of estimation")
print(beta_cost_df.to_string(index=False))

# Print metric table for each method of estimation
print("\nMetric Table:  for each method of estimation")
print(metric_df.to_string(index=False))

"""From the above 2 tables, we can clearly infer that OLS performs better than both LMS and LTS. As the data used has linear relationships without any outliers(Both regressors and responses follows a Normal Distribution). OLS is the best method of estimation followed by LTS then by LMS.
The possible reason for this result is due to optimization and convergence issues. As both OLS and LTS have a faster Optimization and Convergence. But LMS optimization results in a slower and could have global convergence issues. Another reason is that the errors follow a normal distribution, the OLS method tends to perform better.

The metrics table indicates that the Mean Squared Error (MSE), Relative Bias (RB), and Mean Absolute Deviation (MAD) values do not significantly vary between N=20 and N=100. This suggests that these metrics are not heavily dependent on the value of N. However, these metrics are sensitive to hyperparameter tuning, such as adjusting the step-size and the number of iterations, which can lead to improved MSE, MAD, and RB values. For example, the OLS method is heavily dependent on step-size, intially for step size = 10^-2. OLS method didn't converge for N = 100. whereas LTS did. But for step size = 10^-3 and 10^-4 OLS method did converge for N = 100.

# **PREPROCESSING THE REAL DATASET - MEDICAL_INSURANCE.CSV**

Changing Categorical Variables into Numerical Variables.
In the excel sheet,

* For 'sex' variable - I have replaced 'male' with '0' and 'female' with '1'.  
* For 'smoker' Variable - I have replaced 'yes' with '1' and 'no' with '0'.
* For 'region' Variable - I have replaced 'northeast' with '1' , 'northwest' with '2' , 'southeast' with '3' and 'southwest' with '4'.
"""

# Creating a pandas Dataframe for the given Dataset
df = pd.read_csv('medical_insurance.csv')
df

# Select split_ratio
split_ratio = 0.8

# Determining the training dataset size
total_rows = df.shape[0]
train_size = int(total_rows*split_ratio)

# Split data into test and train
train = df[0:train_size]
test = df[train_size:]

# Creating Covariate Matrix (X_train) = (age,sex,bmi,children,smoker,region) for Training Dataset
X_train = train.iloc[:, :-1]
print(X_train)

# Converting Data Frame to a Numpy Array
X_train = np.array(train.iloc[:, :-1])
X_train = (X_train - np.mean(X_train)) / np.std(X_train)
print(X_train)

# Creating Response Matrix (Y_train) = (charges) for Training Dataset
Y_train = train.iloc[:, -1]
print(Y_train)
# Reshaping 1-Dimensional array to 2-Dimensions after converting Data Frame to a Numpy Array
Y_train = np.array(train.iloc[:, -1]).reshape(-1, 1)
# Normalising the Response Matrix
Y_train = (Y_train - np.mean(Y_train)) / np.std(Y_train)
print(Y_train)

# Creating Covariate Matrix (X_test) - (age,sex,bmi,children,smoker,region) for Testing Dataset
X_test = test.iloc[:, :-1]
print(X_test)

# Converting Data Frame to a Numpy Array
X_test = np.array(test.iloc[:, :-1])
X_test = (X_test - np.mean(X_test)) / np.std(X_test)
print(X_test)

# Creating Response Matrix (Y_test) = (charges) for Testing Dataset
Y_test = test.iloc[:, -1]
print(Y_test)

# Reshaping 1-Dimensional array to 2-Dimensions after converting Data Frame to a Numpy Array
Y_test = np.array(test.iloc[:, -1]).reshape(-1, 1)
# Normalising the Response Matrix
Y_test = (Y_test - np.mean(Y_test)) / np.std(Y_test)
print(Y_test[0:10])

"""# **FITTING A LINEAR REGRESSION MODEL TO THE ABOVE DATASET**

Assuming a Linear Regression Model for the above Dataset, We can write the relationship between Response Varaibles (Y) and Regressor Varaibles (X) can be given as:

***$$ Y = \theta_0 X_0 + \theta_1 X_1 + \theta_2 X_2 + \theta_3 X_3 + \theta_4 X_4 + \theta_5 X_5 + \epsilon $$***

where, $\theta_i,$ i = 1,...,6 are the unknown parameters and $\epsilon_i$ is error of the equation & are i.i.d and independent of $x_i$ with $ E(\epsilon_i|x_i) = 0 $.
"""

# Creating a list to store the parameters estimated for each method of estimation ('OLS', 'LMS', 'LTS')
theta_results = []

# For Loop to loop through all 3 methods of estimation ('OLS', 'LMS', 'LTS')
for method in ['OLS', 'LMS', 'LTS']:

    # intializing theta_estimator matrix to shape of number of covarities
    theta_est = np.zeros(((np.shape(X_train)[1])))
    # Estimating the parameters theta for Training Data using Linear Regression Model for each method of estimation with Gradient Descent
    # Creating an instance of the LinearRegression class with a specified method of estimation
    lr = LinearRegression(method=method,l_r = 0.0001, iter=10000)
    # Fitting the linear regression model to the given Training datasets X_train and Y_train using the specified method of estimation
    lr.fit(X_train, Y_train)
    # Storing the estimated parameters (theta) from the fitted model for each method of estimation
    theta_est[:,] = (lr.beta.ravel())
    # Appending the estimated parameters for each method to the list
    theta_results.append([method, theta_est[0], theta_est[1], theta_est[2], theta_est[3], theta_est[4], theta_est[5]])

# Create DataFrame for the estimated parameters (theta) for each method of estimation
theta_df = pd.DataFrame(theta_results, columns=['Method', 'Theta0', 'Theta1', 'Theta2', 'Theta3', 'Theta4', 'Theta5'])

# Print Theta table - The Table of Estimated Parameters by Each Method of Estimation
print("Theta Table: Estimated Parameters by Each Method")
print(theta_df.to_string(index=False))

"""# **EVALUATING THE PERFORMANCE OF EACH METHOD BY MEAN SQUARE ERROR(MSE) ON TEST DATASET**"""

# Creating a list to store the Mean Square Errors(MSE) for each method of estimation ('OLS', 'LMS', 'LTS')
MSE_results = []

# Estimated parameters 'theta' for all 3 methods of estimation
theta_results_dict = {item[0]: item[1:7] for item in theta_results}
theta_OLS = theta_results_dict.get('OLS')
theta_LMS = theta_results_dict.get('LMS')
theta_LTS = theta_results_dict.get('LTS')

# Predicting Response variables (Y_test_bar) for the Covariate Test Dataset (X_test)
Pred_OLS = np.dot(X_test, theta_OLS).reshape(-1,1)
Pred_LMS = np.dot(X_test, theta_LMS).reshape(-1,1)
Pred_LTS = np.dot(X_test, theta_LTS).reshape(-1,1)

# Calculating Mean Squared Error between Predicted Responce Variables (Y_test_bar) vs True Response Variables (Y_test_bar)
N = np.shape(Y_test)[0]
MSE_OLS = (1 / N) * np.sum(((Y_test - Pred_OLS)**2), axis = 0)
MSE_LMS = (1 / N) * np.sum(((Y_test - Pred_LMS)**2), axis = 0)
MSE_LTS = (1 / N) * np.sum(((Y_test - Pred_LTS)**2), axis = 0)

# Appending the calculated MSE values for each method of estimation to the list
MSE_results.append([MSE_OLS[0], MSE_LMS[0], MSE_LTS[0]])

# Create DataFrame for the calculated MSE values for each method of estimation
MSE_df = pd.DataFrame(MSE_results, columns=['MSE_OLS', 'MSE_LMS', 'MSE_LTS'])

# Print MSE table - The Table of calculated MSE values for each method of estimation
print("MSE Table: Calculated Mean Square Error (MSE) Values for Each Method of Estimation")
print(MSE_df.to_string(index=False))

"""From the above results, we can clearly infer that the LTS and LMS methods are far more robust than OLS when the data has outliers.

I have plotted histograms for all columns of the given data and observed outilers in some columns of covariate variable X like bmi and the response variable 'Y' has a lot of outliers.

Also, the scales of data in  Y and X are very different to each other. So, I have normalised both the train and test Covariate and Response Datasets (X & Y).

Finally, we can clearly observe that OLS fails to converge for datasets with outliers, but LTS performs the best due to its similarity with OLS but only considers half of the residuals making it robust. LMS also similar performance only when compared to LTS.

# **CHECKING THE PRESENCE OF OUTLIERS IN THE GIVEN REAL DATA**
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('medical_insurance.csv')

# List of columns for which we need to check outliers
cols = ['age', 'sex', 'bmi', 'children', 'smoker', 'region', 'charges']

# Plotting histograms and boxplots for each column
for col in cols:
    plt.figure(figsize=(16, 4))
    # Histogram
    plt.subplot(1, 2, 1)
    sns.histplot(df[col], kde=True)
    plt.title(f'Histogram of {col}')
    plt.show()